
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Erfan Zare Chavoshi">
      
      
      
        <link rel="prev" href="../generated-modules-mixtral-mixtral_configuration/">
      
      
        <link rel="next" href="../generated-modules-mosaic_mpt-modelling_mpt_flax/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.7">
    
    
      
        <title>Modelling Mixtral Flax - EasyDel</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.f2e4d321.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#modulesmixtralmodelling_mixtral_flax" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="EasyDel" class="md-header__button md-logo" aria-label="EasyDel" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            EasyDel
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Modelling Mixtral Flax
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/erfanzar/EasyDel" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="EasyDel" class="md-nav__button md-logo" aria-label="EasyDel" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    EasyDel
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/erfanzar/EasyDel" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../AvailableModels/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AvailableModels
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../CONTRIBUTING/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CONTRIBUTING
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Data Preprocessing
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Data Preprocessing
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-data_preprocessing-_processor/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Processor
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Bits/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    EasyBIT
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Etils
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Etils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-etils-auto_tx/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Auto Tx
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-etils-configs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Configs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-etils-easystate/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Easystate
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-etils-errors/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Errors
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-etils-etils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Etils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Eval
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Eval
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-eval-lm_eval/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lm Eval
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Examples
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../DataProcessing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DataProcessing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../EasyStateExample/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    EasyState
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Falcon/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Falcon Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../FineTuningExample/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fine Tuning Example
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../JAXServer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    JAXServer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Llama/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Llama Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Llama2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Llama2 Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../LoRA-TransferLearningExample/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LoRA and Transfer Learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Mistral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Mistral Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../MosaicMPT/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MosaicMPT Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../PyTorchServer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PytorchServer
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9" >
        
          
          <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Linen
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            Linen
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-linen-bits/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Bits
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-linen-utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10" checked>
        
          
          <label class="md-nav__link" for="__nav_10" id="__nav_10_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Modules
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_10_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_10">
            <span class="md-nav__icon md-icon"></span>
            Modules
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-auto_easydel_model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Auto Easydel Model
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-easy_attention/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Easy Attention
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-easydel_modelling_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Easydel Modelling Utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_4" >
        
          
          <label class="md-nav__link" for="__nav_10_4" id="__nav_10_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Falcon
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_4">
            <span class="md-nav__icon md-icon"></span>
            Falcon
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-falcon-falcon_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Falcon Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-falcon-modelling_falcon_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Falcon Flax
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-flax_modelling_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Flax Modelling Utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_6" >
        
          
          <label class="md-nav__link" for="__nav_10_6" id="__nav_10_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Gpt J
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_6">
            <span class="md-nav__icon md-icon"></span>
            Gpt J
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-gpt_j-gpt_j_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Gpt J Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-gpt_j-modelling_gpt_j_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Gpt J Flax
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_7" >
        
          
          <label class="md-nav__link" for="__nav_10_7" id="__nav_10_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Gpt Neo X
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_7">
            <span class="md-nav__icon md-icon"></span>
            Gpt Neo X
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-gpt_neo_x-gpt_neo_x_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Gpt Neo X Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-gpt_neo_x-modelling_gpt_neo_x_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Gpt Neo X Flax
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_8" >
        
          
          <label class="md-nav__link" for="__nav_10_8" id="__nav_10_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Gpt2
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_8">
            <span class="md-nav__icon md-icon"></span>
            Gpt2
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-gpt2-gpt2_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Gpt2 Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-gpt2-modelling_gpt2_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Gpt2 Flax
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_9" >
        
          
          <label class="md-nav__link" for="__nav_10_9" id="__nav_10_9_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Llama
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_9">
            <span class="md-nav__icon md-icon"></span>
            Llama
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-llama-llama_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Llama Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-llama-modelling_llama_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Llama Flax
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_10" >
        
          
          <label class="md-nav__link" for="__nav_10_10" id="__nav_10_10_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Lucid Transformer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_10">
            <span class="md-nav__icon md-icon"></span>
            Lucid Transformer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-lucid_transformer-lt_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lt Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-lucid_transformer-modelling_lt_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Lt Flax
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_11" >
        
          
          <label class="md-nav__link" for="__nav_10_11" id="__nav_10_11_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Mamba
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_11">
            <span class="md-nav__icon md-icon"></span>
            Mamba
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-mamba-mamba_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Mamba Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-mamba-modelling_mamba_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Mamba Flax
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_12" >
        
          
          <label class="md-nav__link" for="__nav_10_12" id="__nav_10_12_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Mistral
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_12_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_12">
            <span class="md-nav__icon md-icon"></span>
            Mistral
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-mistral-mistral_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Mistral Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-mistral-modelling_mistral_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Mistral Flax
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_13" checked>
        
          
          <label class="md-nav__link" for="__nav_10_13" id="__nav_10_13_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Mixtral
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_13_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_10_13">
            <span class="md-nav__icon md-icon"></span>
            Mixtral
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-mixtral-mixtral_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Mixtral Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Modelling Mixtral Flax
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Modelling Mixtral Flax
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax" class="md-nav__link">
    <span class="md-ellipsis">
      modelling_mixtral_flax
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralAttention" class="md-nav__link">
    <span class="md-ellipsis">
      FlaxMixtralAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FlaxMixtralAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralAttention.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayer" class="md-nav__link">
    <span class="md-ellipsis">
      FlaxMixtralDecoderLayer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FlaxMixtralDecoderLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayer.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayerCollection" class="md-nav__link">
    <span class="md-ellipsis">
      FlaxMixtralDecoderLayerCollection
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FlaxMixtralDecoderLayerCollection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayerCollection.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralForCausalLM" class="md-nav__link">
    <span class="md-ellipsis">
      FlaxMixtralForCausalLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FlaxMixtralForCausalLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralForCausalLM.prepare_inputs_for_generation" class="md-nav__link">
    <span class="md-ellipsis">
      prepare_inputs_for_generation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralSparseMoeBlock" class="md-nav__link">
    <span class="md-ellipsis">
      FlaxMixtralSparseMoeBlock
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel" class="md-nav__link">
    <span class="md-ellipsis">
      MixtralPreTrainedModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MixtralPreTrainedModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel.init_weights" class="md-nav__link">
    <span class="md-ellipsis">
      init_weights
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_14" >
        
          
          <label class="md-nav__link" for="__nav_10_14" id="__nav_10_14_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Mosaic Mpt
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_14_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_14">
            <span class="md-nav__icon md-icon"></span>
            Mosaic Mpt
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-mosaic_mpt-modelling_mpt_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Mpt Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-mosaic_mpt-mosaic_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Mosaic Configuration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_15" >
        
          
          <label class="md-nav__link" for="__nav_10_15" id="__nav_10_15_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Opt
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_15_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_15">
            <span class="md-nav__icon md-icon"></span>
            Opt
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-opt-modelling_opt_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Opt Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-opt-opt_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Opt Configuration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_16" >
        
          
          <label class="md-nav__link" for="__nav_10_16" id="__nav_10_16_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Palm
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_16_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_16">
            <span class="md-nav__icon md-icon"></span>
            Palm
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-palm-modelling_palm_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Palm Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-palm-palm_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Palm Configuration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_17" >
        
          
          <label class="md-nav__link" for="__nav_10_17" id="__nav_10_17_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Phi
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_17_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_17">
            <span class="md-nav__icon md-icon"></span>
            Phi
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-phi-modelling_phi_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Phi Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-phi-phi_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Phi Configuration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_18" >
        
          
          <label class="md-nav__link" for="__nav_10_18" id="__nav_10_18_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Qwen2
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_18_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_18">
            <span class="md-nav__icon md-icon"></span>
            Qwen2
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-qwen2-modelling_qwen_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Qwen Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-qwen2-qwen_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Qwen Configuration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_19" >
        
          
          <label class="md-nav__link" for="__nav_10_19" id="__nav_10_19_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Roberta
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_19_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_19">
            <span class="md-nav__icon md-icon"></span>
            Roberta
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-roberta-modelling_roberta_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Roberta Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-roberta-roberta_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Roberta Configuration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_20" >
        
          
          <label class="md-nav__link" for="__nav_10_20" id="__nav_10_20_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    T5
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_20_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_20">
            <span class="md-nav__icon md-icon"></span>
            T5
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-t5-modelling_t5_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling T5 Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-t5-t5_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    T5 Configuration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_11" >
        
          
          <label class="md-nav__link" for="__nav_11" id="__nav_11_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Partitioning
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_11">
            <span class="md-nav__icon md-icon"></span>
            Partitioning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-partitioning-partitioner/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Partitioner
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_12" >
        
          
          <label class="md-nav__link" for="__nav_12" id="__nav_12_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Reinforcement Learning
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_12_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_12">
            <span class="md-nav__icon md-icon"></span>
            Reinforcement Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-reinforcement_learning-core/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Core
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_12_2" >
        
          
          <label class="md-nav__link" for="__nav_12_2" id="__nav_12_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Models
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_12_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_12_2">
            <span class="md-nav__icon md-icon"></span>
            Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-reinforcement_learning-models-modelling_casual_language_rl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Casual Language Rl
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_12_3" >
        
          
          <label class="md-nav__link" for="__nav_12_3" id="__nav_12_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Trainer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_12_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_12_3">
            <span class="md-nav__icon md-icon"></span>
            Trainer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-reinforcement_learning-trainer-base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-reinforcement_learning-trainer-dpo_trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dpo Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-reinforcement_learning-trainer-partitioner_config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Partitioner Config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-reinforcement_learning-trainer-ppo_config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Ppo Config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-reinforcement_learning-trainer-ppo_trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Ppo Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-reinforcement_learning-trainer-training_configs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Training Configs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-reinforcement_learning-trainer-utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_12_4" >
        
          
          <label class="md-nav__link" for="__nav_12_4" id="__nav_12_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Utils
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_12_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_12_4">
            <span class="md-nav__icon md-icon"></span>
            Utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-reinforcement_learning-utils-collectors/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Collectors
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13" >
        
          
          <label class="md-nav__link" for="__nav_13" id="__nav_13_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Serve
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_13_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13">
            <span class="md-nav__icon md-icon"></span>
            Serve
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13_1" >
        
          
          <label class="md-nav__link" for="__nav_13_1" id="__nav_13_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Api
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_13_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13_1">
            <span class="md-nav__icon md-icon"></span>
            Api
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-serve-api-client/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Client
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-serve-api-configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-serve-api-dantics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dantics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-serve-api-serve/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Serve
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-serve-gradio_user_interface_base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Gradio User Interface Base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-serve-jax_serve/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Jax Serve
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-serve-torch_serve/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Torch Serve
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-serve-utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_14" >
        
          
          <label class="md-nav__link" for="__nav_14" id="__nav_14_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Smi
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_14_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_14">
            <span class="md-nav__icon md-icon"></span>
            Smi
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-smi-smi/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Smi
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_15" >
        
          
          <label class="md-nav__link" for="__nav_15" id="__nav_15_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Trainer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_15_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_15">
            <span class="md-nav__icon md-icon"></span>
            Trainer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-trainer-base_trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Base Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-trainer-causal_language_model_trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Causal Language Model Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-trainer-training_configurations/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Training Configurations
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-trainer-utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_16" >
        
          
          <label class="md-nav__link" for="__nav_16" id="__nav_16_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Transform
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_16_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_16">
            <span class="md-nav__icon md-icon"></span>
            Transform
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-transform-easydel_transform/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Easydel Transform
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-transform-falcon/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Falcon
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-transform-llama/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Llama
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-transform-mistral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Mistral
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-transform-mpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Mpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-transform-utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_17" >
        
          
          <label class="md-nav__link" for="__nav_17" id="__nav_17_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Utils
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_17_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_17">
            <span class="md-nav__icon md-icon"></span>
            Utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-utils-checker/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Checker
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-utils-prompters/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompters
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-utils-tensor_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tensor Utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-utils-utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Install/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    install
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax" class="md-nav__link">
    <span class="md-ellipsis">
      modelling_mixtral_flax
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralAttention" class="md-nav__link">
    <span class="md-ellipsis">
      FlaxMixtralAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FlaxMixtralAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralAttention.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayer" class="md-nav__link">
    <span class="md-ellipsis">
      FlaxMixtralDecoderLayer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FlaxMixtralDecoderLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayer.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayerCollection" class="md-nav__link">
    <span class="md-ellipsis">
      FlaxMixtralDecoderLayerCollection
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FlaxMixtralDecoderLayerCollection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayerCollection.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralForCausalLM" class="md-nav__link">
    <span class="md-ellipsis">
      FlaxMixtralForCausalLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FlaxMixtralForCausalLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralForCausalLM.prepare_inputs_for_generation" class="md-nav__link">
    <span class="md-ellipsis">
      prepare_inputs_for_generation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralSparseMoeBlock" class="md-nav__link">
    <span class="md-ellipsis">
      FlaxMixtralSparseMoeBlock
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel" class="md-nav__link">
    <span class="md-ellipsis">
      MixtralPreTrainedModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MixtralPreTrainedModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel.init_weights" class="md-nav__link">
    <span class="md-ellipsis">
      init_weights
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="modulesmixtralmodelling_mixtral_flax">modules.mixtral.modelling_mixtral_flax</h1>


<div class="doc doc-object doc-module">



<a id="lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax"></a>
  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralAttention" class="doc doc-heading">
          <code>FlaxMixtralAttention</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="flax.linen.Module">Module</span></code></p>


            <details class="quote">
              <summary>Source code in <code>lib/python/EasyDel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">FlaxMixtralAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">MixtralConfig</span>
    <span class="n">layer_index</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="n">param_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="n">precision</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">Precision</span><span class="p">]</span>
                        <span class="p">]</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">Precision</span><span class="p">(</span><span class="s2">&quot;fastest&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_key_value_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_position_embeddings</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span>

        <span class="n">dense</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">,</span>
            <span class="n">use_bias</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;attention_bias&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">param_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dtype</span><span class="p">,</span>
            <span class="n">precision</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span>
            <span class="n">kernel_init</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">normal</span><span class="p">(),</span>
            <span class="o">**</span><span class="n">get_dot_general_by_bits</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">bits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">easy_method</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span> <span class="o">=</span> <span class="n">dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rotary</span> <span class="o">=</span> <span class="n">FlaxMixtralRotaryEmbedding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_performer</span> <span class="o">=</span> <span class="n">EasyAttention</span><span class="p">(</span>
            <span class="n">attn_type</span><span class="o">=</span><span class="s2">&quot;normal&quot;</span><span class="p">,</span>
            <span class="n">block_k_major</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_k_major</span><span class="p">,</span>
            <span class="n">block_b</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_b</span><span class="p">,</span>
            <span class="n">block_q</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_q</span><span class="p">,</span>
            <span class="n">block_k</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_k</span><span class="p">,</span>
            <span class="n">block_q_major_dkv</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_q_major_dkv</span><span class="p">,</span>
            <span class="n">block_k_major_dkv</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_k_major_dkv</span><span class="p">,</span>
            <span class="n">block_k_major_dq</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_k_major_dq</span><span class="p">,</span>
            <span class="n">block_k_dkv</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_k_dkv</span><span class="p">,</span>
            <span class="n">block_q_dkv</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_q_dkv</span><span class="p">,</span>
            <span class="n">block_q_dq</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_q_dq</span><span class="p">,</span>
            <span class="n">block_k_dq</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_k_dq</span><span class="p">,</span>
            <span class="n">num_attention_heads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span>
            <span class="n">attention_dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span><span class="p">,</span>
            <span class="n">head_dims</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
            <span class="n">attention_partition_spec</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attention_partition_spec</span><span class="p">,</span>
            <span class="n">use_shard_map</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_shard_map</span><span class="p">,</span>
            <span class="n">precision</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span>
            <span class="n">force_float32_tpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">attn_mechanism</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_mechanism</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">bias_partition_spec</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">bias_partition_spec</span><span class="p">,</span>
            <span class="n">key_partition_spec</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">key_partition_spec</span><span class="p">,</span>
            <span class="n">query_partition_spec</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">query_partition_spec</span><span class="p">,</span>
            <span class="n">value_partition_spec</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">value_partition_spec</span><span class="p">,</span>
            <span class="n">mesh</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">jax_mesh</span><span class="p">(),</span>
            <span class="n">sm_scale</span><span class="o">=</span><span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_t</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">apply_rotary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">freq_cis</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">):</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span>
                              <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span>
                              <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_t</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="n">query</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary</span><span class="p">(</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span> <span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">,</span> <span class="n">freq_cis</span><span class="o">=</span><span class="n">freq_cis</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">repeat_kv_bnsh</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">repeat_kv_bnsh</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_t</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_merge_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,))</span>

    <span class="nd">@nn</span><span class="o">.</span><span class="n">compact</span>
    <span class="k">def</span> <span class="nf">_concatenate_to_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The _concatenate_to_cache function is used to concatenate the key and value vectors</span>
<span class="sd">        of a query with those of previous queries. This allows for the attention mechanism to</span>
<span class="sd">        look at all previous queries when computing its output. The function takes in three</span>
<span class="sd">        arguments: key, value, and query. It also uses two variables that are stored in the cache:</span>
<span class="sd">        cached_key and cached_value.</span>

<span class="sd">        :param self: Access the variables stored in the cache</span>
<span class="sd">        :param key: Store the keys of the encoder-decoder attention</span>
<span class="sd">        :param value: Initialize the cached_value variable</span>
<span class="sd">        :param query: Determine the number of cache vectors to update</span>
<span class="sd">        :param attention_mask: Mask out the padded vectors in the cache</span>
<span class="sd">        :return: The key, value and attention_mask</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">is_initialized</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_variable</span><span class="p">(</span><span class="s2">&quot;cache&quot;</span><span class="p">,</span> <span class="s2">&quot;cached_key&quot;</span><span class="p">)</span>
        <span class="n">cached_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">variable</span><span class="p">(</span>
            <span class="s2">&quot;cache&quot;</span><span class="p">,</span> <span class="s2">&quot;cached_key&quot;</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">cached_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">variable</span><span class="p">(</span>
            <span class="s2">&quot;cache&quot;</span><span class="p">,</span> <span class="s2">&quot;cached_value&quot;</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">,</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">cache_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">variable</span><span class="p">(</span>
            <span class="s2">&quot;cache&quot;</span><span class="p">,</span> <span class="s2">&quot;cache_index&quot;</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">is_initialized</span><span class="p">:</span>
            <span class="o">*</span><span class="n">batch_dims</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">depth_per_head</span> <span class="o">=</span> <span class="n">cached_key</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">cur_index</span> <span class="o">=</span> <span class="n">cache_index</span><span class="o">.</span><span class="n">value</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">cur_index</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">dynamic_update_slice</span><span class="p">(</span><span class="n">cached_key</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">dynamic_update_slice</span><span class="p">(</span>
                <span class="n">cached_value</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
            <span class="n">cached_key</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">key</span>
            <span class="n">cached_value</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>
            <span class="n">num_updated_cache_vectors</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">cache_index</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">cache_index</span><span class="o">.</span><span class="n">value</span> <span class="o">+</span> <span class="n">num_updated_cache_vectors</span>

            <span class="n">pad_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span>
                <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_length</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">cur_index</span> <span class="o">+</span> <span class="n">num_updated_cache_vectors</span><span class="p">,</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_updated_cache_vectors</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">combine_masks</span><span class="p">(</span><span class="n">pad_mask</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attention_mask</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">freq_cis</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">causal_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">deterministic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
            <span class="n">init_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The __call__ function is the main function of a JAX module.</span>
<span class="sd">        It defines how the module behaves when called as a function, and it&#39;s what you&#39;ll use to call your model in practice.</span>
<span class="sd">        The __call__ method takes an input tensor (x) and returns an output tensor (y).</span>
<span class="sd">        In this case, we&#39;re defining our model to be a simple linear layer with no activation: y = x @ w + b.</span>

<span class="sd">        :param self: Refer to the object itself</span>
<span class="sd">        :param hidden_states: chex.Array: Pass in the hidden state of the model</span>
<span class="sd">        :param freq_cis: chex.Array: Create the apply_rotary variable</span>
<span class="sd">        :param attention_mask: chex.Array: Mask the attention weights</span>
<span class="sd">        :param causal_mask: chex.Array: Mask the attention weights</span>
<span class="sd">        :param position_ids: chex.Array: Specify the position of each token in a sequence</span>
<span class="sd">        :param deterministic: bool: Determine whether to use dropout or not</span>
<span class="sd">        :param init_cache: bool: Initialize the cache</span>
<span class="sd">        :param output_attentions: bool: Determine whether to return the attention weights</span>
<span class="sd">        :return: A tuple of (out, attn_output)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">query_state</span><span class="p">,</span> <span class="n">key_state</span><span class="p">,</span> <span class="n">value_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_pjit_attention_force</span><span class="p">:</span>
            <span class="n">query_state</span> <span class="o">=</span> <span class="n">with_sharding_constraint</span><span class="p">(</span>
                <span class="n">query_state</span><span class="p">,</span> <span class="n">PartitionSpec</span><span class="p">((</span><span class="s2">&quot;dp&quot;</span><span class="p">,</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">),</span> <span class="s2">&quot;sp&quot;</span><span class="p">,</span> <span class="s2">&quot;tp&quot;</span><span class="p">))</span>
            <span class="n">key_state</span> <span class="o">=</span> <span class="n">with_sharding_constraint</span><span class="p">(</span>
                <span class="n">key_state</span><span class="p">,</span> <span class="n">PartitionSpec</span><span class="p">((</span><span class="s2">&quot;dp&quot;</span><span class="p">,</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">),</span> <span class="s2">&quot;sp&quot;</span><span class="p">,</span> <span class="s2">&quot;tp&quot;</span><span class="p">))</span>
            <span class="n">value_state</span> <span class="o">=</span> <span class="n">with_sharding_constraint</span><span class="p">(</span>
                <span class="n">value_state</span><span class="p">,</span> <span class="n">PartitionSpec</span><span class="p">((</span><span class="s2">&quot;dp&quot;</span><span class="p">,</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">),</span> <span class="s2">&quot;sp&quot;</span><span class="p">,</span> <span class="s2">&quot;tp&quot;</span><span class="p">))</span>

        <span class="n">query_state</span> <span class="o">=</span> <span class="n">query_state</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">key_state</span> <span class="o">=</span> <span class="n">key_state</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">value_state</span> <span class="o">=</span> <span class="n">value_state</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

        <span class="n">query_state</span><span class="p">,</span> <span class="n">key_state</span><span class="p">,</span> <span class="n">value_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary</span><span class="p">(</span>
            <span class="n">query</span><span class="o">=</span><span class="n">query_state</span><span class="p">,</span>
            <span class="n">key</span><span class="o">=</span><span class="n">key_state</span><span class="p">,</span>
            <span class="n">value</span><span class="o">=</span><span class="n">value_state</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">freq_cis</span><span class="o">=</span><span class="n">freq_cis</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">sequence_length</span><span class="o">=</span><span class="n">sequence_length</span>
        <span class="p">)</span>

        <span class="n">assert_msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;num_attention_heads repeat wont work likely</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;INFO :</span><span class="se">\n\t</span><span class="s2">repeat_kv_bnsh Used with num_key_value_groups = </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span><span class="si">}</span><span class="se">\n\t</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;NH : </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="si">}</span><span class="s2"> KVH : </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="k">assert</span> <span class="n">query_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">assert_msg</span>
        <span class="k">assert</span> <span class="n">key_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">assert_msg</span>
        <span class="k">assert</span> <span class="n">value_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">assert_msg</span>

        <span class="n">query_length</span><span class="p">,</span> <span class="n">key_length</span> <span class="o">=</span> <span class="n">query_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">key_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_variable</span><span class="p">(</span><span class="s2">&quot;cache&quot;</span><span class="p">,</span> <span class="s2">&quot;cached_key&quot;</span><span class="p">):</span>
            <span class="n">mask_shift</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">variables</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">][</span><span class="s2">&quot;cache_index&quot;</span><span class="p">]</span>
            <span class="n">max_decoder_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">variables</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">][</span><span class="s2">&quot;cached_key&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">dynamic_slice</span><span class="p">(</span>
                <span class="n">causal_mask</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">mask_shift</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                                                     <span class="n">query_length</span><span class="p">,</span> <span class="n">max_decoder_length</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">causal_mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">query_length</span><span class="p">,</span> <span class="p">:</span><span class="n">key_length</span><span class="p">]</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span>
            <span class="n">causal_mask</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,)</span> <span class="o">+</span> <span class="n">causal_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
            <span class="n">attention_mask</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)),</span> <span class="n">causal_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">combine_masks</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">causal_mask</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>

        <span class="n">dropout_rng</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">deterministic</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="n">dropout_rng</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_rng</span><span class="p">(</span><span class="s2">&quot;dropout&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_variable</span><span class="p">(</span><span class="s2">&quot;cache&quot;</span><span class="p">,</span> <span class="s2">&quot;cached_key&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">init_cache</span><span class="p">:</span>
            <span class="n">key_state</span><span class="p">,</span> <span class="n">value_state</span><span class="p">,</span> <span class="n">attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concatenate_to_cache</span><span class="p">(</span>
                <span class="n">key_state</span><span class="p">,</span>
                <span class="n">value_state</span><span class="p">,</span>
                <span class="n">query_state</span><span class="p">,</span>
                <span class="n">attention_mask</span>
            <span class="p">)</span>

        <span class="n">attention_bias</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">select</span><span class="p">(</span>
            <span class="n">attention_mask</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="n">query_state</span><span class="p">,</span> <span class="n">key_state</span><span class="p">,</span> <span class="n">value_state</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">a</span><span class="p">:</span> <span class="n">a</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
            <span class="p">[</span><span class="n">query_state</span><span class="p">,</span> <span class="n">key_state</span><span class="p">,</span> <span class="n">value_state</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="n">query_length</span><span class="p">,</span> <span class="n">key_length</span> <span class="o">=</span> <span class="n">query_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">key_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>

        <span class="n">attentions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_performer</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span>
            <span class="n">query_states</span><span class="o">=</span><span class="n">query_state</span><span class="p">,</span>
            <span class="n">key_states</span><span class="o">=</span><span class="n">key_state</span><span class="p">,</span>
            <span class="n">value_states</span><span class="o">=</span><span class="n">value_state</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="n">attention_bias</span><span class="p">,</span>
            <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">use_pjit_attention_force</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_pjit_attention_force</span><span class="p">,</span>
            <span class="n">dropout_rng</span><span class="o">=</span><span class="n">dropout_rng</span><span class="p">,</span>
            <span class="n">deterministic</span><span class="o">=</span><span class="n">deterministic</span><span class="p">,</span>
            <span class="n">query_sequence_length</span><span class="o">=</span><span class="n">query_length</span><span class="p">,</span>
            <span class="n">key_value_sequence_length</span><span class="o">=</span><span class="n">key_length</span><span class="p">,</span>
            <span class="n">uses_cache</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">has_variable</span><span class="p">(</span><span class="s2">&quot;cache&quot;</span><span class="p">,</span> <span class="s2">&quot;cached_key&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">init_cache</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">attentions</span><span class="o">.</span><span class="n">attention_outputs</span> <span class="o">=</span> <span class="n">attentions</span><span class="o">.</span><span class="n">attention_outputs</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
            <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merge_heads</span><span class="p">(</span><span class="n">attentions</span><span class="o">.</span><span class="n">attention_outputs</span><span class="p">)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">attn_output</span><span class="p">,</span> <span class="n">attentions</span><span class="o">.</span><span class="n">attention_weights</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralAttention.__call__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">freq_cis</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">causal_mask</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">init_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>The <strong>call</strong> function is the main function of a JAX module.
It defines how the module behaves when called as a function, and it's what you'll use to call your model in practice.
The <strong>call</strong> method takes an input tensor (x) and returns an output tensor (y).
In this case, we're defining our model to be a simple linear layer with no activation: y = x @ w + b.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>self</code></td>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Refer to the object itself</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>hidden_states</code></td>
          <td>
                <code><span title="chex.Array">Array</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>chex.Array: Pass in the hidden state of the model</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>freq_cis</code></td>
          <td>
                <code><span title="chex.Array">Array</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>chex.Array: Create the apply_rotary variable</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>attention_mask</code></td>
          <td>
                <code><span title="chex.Array">Array</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>chex.Array: Mask the attention weights</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>causal_mask</code></td>
          <td>
                <code><span title="chex.Array">Array</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>chex.Array: Mask the attention weights</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>position_ids</code></td>
          <td>
                <code><span title="chex.Array">Array</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>chex.Array: Specify the position of each token in a sequence</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>deterministic</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>bool: Determine whether to use dropout or not</p>
            </div>
          </td>
          <td>
                <code>True</code>
          </td>
        </tr>
        <tr>
          <td><code>init_cache</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>bool: Initialize the cache</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>output_attentions</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>bool: Determine whether to return the attention weights</p>
            </div>
          </td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A tuple of (out, attn_output)</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>lib/python/EasyDel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">freq_cis</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">causal_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">deterministic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">init_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The __call__ function is the main function of a JAX module.</span>
<span class="sd">    It defines how the module behaves when called as a function, and it&#39;s what you&#39;ll use to call your model in practice.</span>
<span class="sd">    The __call__ method takes an input tensor (x) and returns an output tensor (y).</span>
<span class="sd">    In this case, we&#39;re defining our model to be a simple linear layer with no activation: y = x @ w + b.</span>

<span class="sd">    :param self: Refer to the object itself</span>
<span class="sd">    :param hidden_states: chex.Array: Pass in the hidden state of the model</span>
<span class="sd">    :param freq_cis: chex.Array: Create the apply_rotary variable</span>
<span class="sd">    :param attention_mask: chex.Array: Mask the attention weights</span>
<span class="sd">    :param causal_mask: chex.Array: Mask the attention weights</span>
<span class="sd">    :param position_ids: chex.Array: Specify the position of each token in a sequence</span>
<span class="sd">    :param deterministic: bool: Determine whether to use dropout or not</span>
<span class="sd">    :param init_cache: bool: Initialize the cache</span>
<span class="sd">    :param output_attentions: bool: Determine whether to return the attention weights</span>
<span class="sd">    :return: A tuple of (out, attn_output)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">query_state</span><span class="p">,</span> <span class="n">key_state</span><span class="p">,</span> <span class="n">value_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span>
        <span class="n">hidden_states</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_pjit_attention_force</span><span class="p">:</span>
        <span class="n">query_state</span> <span class="o">=</span> <span class="n">with_sharding_constraint</span><span class="p">(</span>
            <span class="n">query_state</span><span class="p">,</span> <span class="n">PartitionSpec</span><span class="p">((</span><span class="s2">&quot;dp&quot;</span><span class="p">,</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">),</span> <span class="s2">&quot;sp&quot;</span><span class="p">,</span> <span class="s2">&quot;tp&quot;</span><span class="p">))</span>
        <span class="n">key_state</span> <span class="o">=</span> <span class="n">with_sharding_constraint</span><span class="p">(</span>
            <span class="n">key_state</span><span class="p">,</span> <span class="n">PartitionSpec</span><span class="p">((</span><span class="s2">&quot;dp&quot;</span><span class="p">,</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">),</span> <span class="s2">&quot;sp&quot;</span><span class="p">,</span> <span class="s2">&quot;tp&quot;</span><span class="p">))</span>
        <span class="n">value_state</span> <span class="o">=</span> <span class="n">with_sharding_constraint</span><span class="p">(</span>
            <span class="n">value_state</span><span class="p">,</span> <span class="n">PartitionSpec</span><span class="p">((</span><span class="s2">&quot;dp&quot;</span><span class="p">,</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">),</span> <span class="s2">&quot;sp&quot;</span><span class="p">,</span> <span class="s2">&quot;tp&quot;</span><span class="p">))</span>

    <span class="n">query_state</span> <span class="o">=</span> <span class="n">query_state</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
    <span class="n">key_state</span> <span class="o">=</span> <span class="n">key_state</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
    <span class="n">value_state</span> <span class="o">=</span> <span class="n">value_state</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

    <span class="n">query_state</span><span class="p">,</span> <span class="n">key_state</span><span class="p">,</span> <span class="n">value_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary</span><span class="p">(</span>
        <span class="n">query</span><span class="o">=</span><span class="n">query_state</span><span class="p">,</span>
        <span class="n">key</span><span class="o">=</span><span class="n">key_state</span><span class="p">,</span>
        <span class="n">value</span><span class="o">=</span><span class="n">value_state</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
        <span class="n">freq_cis</span><span class="o">=</span><span class="n">freq_cis</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">sequence_length</span><span class="o">=</span><span class="n">sequence_length</span>
    <span class="p">)</span>

    <span class="n">assert_msg</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;num_attention_heads repeat wont work likely</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="sa">f</span><span class="s2">&quot;INFO :</span><span class="se">\n\t</span><span class="s2">repeat_kv_bnsh Used with num_key_value_groups = </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span><span class="si">}</span><span class="se">\n\t</span><span class="s2">&quot;</span>
        <span class="sa">f</span><span class="s2">&quot;NH : </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="si">}</span><span class="s2"> KVH : </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="k">assert</span> <span class="n">query_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">assert_msg</span>
    <span class="k">assert</span> <span class="n">key_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">assert_msg</span>
    <span class="k">assert</span> <span class="n">value_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">assert_msg</span>

    <span class="n">query_length</span><span class="p">,</span> <span class="n">key_length</span> <span class="o">=</span> <span class="n">query_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">key_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_variable</span><span class="p">(</span><span class="s2">&quot;cache&quot;</span><span class="p">,</span> <span class="s2">&quot;cached_key&quot;</span><span class="p">):</span>
        <span class="n">mask_shift</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">variables</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">][</span><span class="s2">&quot;cache_index&quot;</span><span class="p">]</span>
        <span class="n">max_decoder_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">variables</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">][</span><span class="s2">&quot;cached_key&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">dynamic_slice</span><span class="p">(</span>
            <span class="n">causal_mask</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">mask_shift</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                                                 <span class="n">query_length</span><span class="p">,</span> <span class="n">max_decoder_length</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">causal_mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">query_length</span><span class="p">,</span> <span class="p">:</span><span class="n">key_length</span><span class="p">]</span>

    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span>
        <span class="n">causal_mask</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,)</span> <span class="o">+</span> <span class="n">causal_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
        <span class="n">attention_mask</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)),</span> <span class="n">causal_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">combine_masks</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">causal_mask</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>

    <span class="n">dropout_rng</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">deterministic</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="n">dropout_rng</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_rng</span><span class="p">(</span><span class="s2">&quot;dropout&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_variable</span><span class="p">(</span><span class="s2">&quot;cache&quot;</span><span class="p">,</span> <span class="s2">&quot;cached_key&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">init_cache</span><span class="p">:</span>
        <span class="n">key_state</span><span class="p">,</span> <span class="n">value_state</span><span class="p">,</span> <span class="n">attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concatenate_to_cache</span><span class="p">(</span>
            <span class="n">key_state</span><span class="p">,</span>
            <span class="n">value_state</span><span class="p">,</span>
            <span class="n">query_state</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>

    <span class="n">attention_bias</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">select</span><span class="p">(</span>
        <span class="n">attention_mask</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="n">query_state</span><span class="p">,</span> <span class="n">key_state</span><span class="p">,</span> <span class="n">value_state</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">a</span><span class="p">:</span> <span class="n">a</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
        <span class="p">[</span><span class="n">query_state</span><span class="p">,</span> <span class="n">key_state</span><span class="p">,</span> <span class="n">value_state</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="n">query_length</span><span class="p">,</span> <span class="n">key_length</span> <span class="o">=</span> <span class="n">query_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">key_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>

    <span class="n">attentions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_performer</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span>
        <span class="n">query_states</span><span class="o">=</span><span class="n">query_state</span><span class="p">,</span>
        <span class="n">key_states</span><span class="o">=</span><span class="n">key_state</span><span class="p">,</span>
        <span class="n">value_states</span><span class="o">=</span><span class="n">value_state</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">attention_bias</span><span class="p">,</span>
        <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">use_pjit_attention_force</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_pjit_attention_force</span><span class="p">,</span>
        <span class="n">dropout_rng</span><span class="o">=</span><span class="n">dropout_rng</span><span class="p">,</span>
        <span class="n">deterministic</span><span class="o">=</span><span class="n">deterministic</span><span class="p">,</span>
        <span class="n">query_sequence_length</span><span class="o">=</span><span class="n">query_length</span><span class="p">,</span>
        <span class="n">key_value_sequence_length</span><span class="o">=</span><span class="n">key_length</span><span class="p">,</span>
        <span class="n">uses_cache</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">has_variable</span><span class="p">(</span><span class="s2">&quot;cache&quot;</span><span class="p">,</span> <span class="s2">&quot;cached_key&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">init_cache</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">attentions</span><span class="o">.</span><span class="n">attention_outputs</span> <span class="o">=</span> <span class="n">attentions</span><span class="o">.</span><span class="n">attention_outputs</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
        <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merge_heads</span><span class="p">(</span><span class="n">attentions</span><span class="o">.</span><span class="n">attention_outputs</span><span class="p">)</span>
    <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">attn_output</span><span class="p">,</span> <span class="n">attentions</span><span class="o">.</span><span class="n">attention_weights</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayer" class="doc doc-heading">
          <code>FlaxMixtralDecoderLayer</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="flax.linen.Module">Module</span></code></p>


            <details class="quote">
              <summary>Source code in <code>lib/python/EasyDel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">FlaxMixtralDecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">MixtralConfig</span>
    <span class="n">layer_index</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="n">param_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="n">precision</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">Precision</span><span class="p">]</span>
                        <span class="p">]</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">Precision</span><span class="p">(</span><span class="s2">&quot;fastest&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">FlaxMixtralAttention</span><span class="p">(</span>
            <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
            <span class="n">layer_index</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_index</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">param_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dtype</span><span class="p">,</span>
            <span class="n">precision</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_sparse_moe</span> <span class="o">=</span> <span class="n">FlaxMixtralSparseMoeBlock</span><span class="p">(</span>
            <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">param_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dtype</span><span class="p">,</span>
            <span class="n">precision</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_layernorm</span> <span class="o">=</span> <span class="n">MixtralRMSNorm</span><span class="p">(</span>
            <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">rms_norm_eps</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">param_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dtype</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_attention_layernorm</span> <span class="o">=</span> <span class="n">MixtralRMSNorm</span><span class="p">(</span>
            <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">rms_norm_eps</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">param_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dtype</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">freq_cis</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">causal_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">deterministic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
            <span class="n">init_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
            <span class="n">output_router_logits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The __call__ function is the main function of a TransformerEncoderLayer.</span>
<span class="sd">        It takes in the following arguments:</span>
<span class="sd">            hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.</span>
<span class="sd">            freq_cis (chex.Array): A tensor containing frequency-domain representations of each token&#39;s context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num</span>

<span class="sd">        :param self: Represent the instance of the class</span>
<span class="sd">        :param hidden_states: chex.Array: Represent the input to the encoder layer</span>
<span class="sd">        :param freq_cis: chex.Array: Pass the frequency information to the attention layer</span>
<span class="sd">        :param attention_mask: chex.Array: Mask out the attention weights for certain positions</span>
<span class="sd">        :param causal_mask: chex.Array: Mask the future tokens</span>
<span class="sd">        :param position_ids: chex.Array: Indicate the position of each token in the sequence</span>
<span class="sd">        :param deterministic: bool: Determine whether to use dropout or not</span>
<span class="sd">        :param init_cache: bool: Initialize the cache for the self-attention layer</span>
<span class="sd">        :param output_attentions: bool: Determine whether to return the attention weights or not</span>
<span class="sd">        :return: A tuple of hidden_states and attention_output</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_layernorm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">self_attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">freq_cis</span><span class="o">=</span><span class="n">freq_cis</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">causal_mask</span><span class="o">=</span><span class="n">causal_mask</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">deterministic</span><span class="o">=</span><span class="n">deterministic</span><span class="p">,</span>
            <span class="n">init_cache</span><span class="o">=</span><span class="n">init_cache</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span>
        <span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_attention_layernorm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">router_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_sparse_moe</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">self_attn_weights</span><span class="p">,)</span>
        <span class="k">if</span> <span class="n">output_router_logits</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">router_logits</span><span class="p">,)</span>
        <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayer.__call__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">freq_cis</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">causal_mask</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">init_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_router_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>The <strong>call</strong> function is the main function of a TransformerEncoderLayer.
It takes in the following arguments:
    hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.
    freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>self</code></td>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Represent the instance of the class</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>hidden_states</code></td>
          <td>
                <code><span title="chex.Array">Array</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>chex.Array: Represent the input to the encoder layer</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>freq_cis</code></td>
          <td>
                <code><span title="chex.Array">Array</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>chex.Array: Pass the frequency information to the attention layer</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>attention_mask</code></td>
          <td>
                <code><span title="chex.Array">Array</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>chex.Array: Mask out the attention weights for certain positions</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>causal_mask</code></td>
          <td>
                <code><span title="chex.Array">Array</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>chex.Array: Mask the future tokens</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>position_ids</code></td>
          <td>
                <code><span title="chex.Array">Array</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>chex.Array: Indicate the position of each token in the sequence</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>deterministic</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>bool: Determine whether to use dropout or not</p>
            </div>
          </td>
          <td>
                <code>True</code>
          </td>
        </tr>
        <tr>
          <td><code>init_cache</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>bool: Initialize the cache for the self-attention layer</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>output_attentions</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>bool: Determine whether to return the attention weights or not</p>
            </div>
          </td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A tuple of hidden_states and attention_output</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>lib/python/EasyDel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">freq_cis</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">causal_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">deterministic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">init_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">output_router_logits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The __call__ function is the main function of a TransformerEncoderLayer.</span>
<span class="sd">    It takes in the following arguments:</span>
<span class="sd">        hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.</span>
<span class="sd">        freq_cis (chex.Array): A tensor containing frequency-domain representations of each token&#39;s context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num</span>

<span class="sd">    :param self: Represent the instance of the class</span>
<span class="sd">    :param hidden_states: chex.Array: Represent the input to the encoder layer</span>
<span class="sd">    :param freq_cis: chex.Array: Pass the frequency information to the attention layer</span>
<span class="sd">    :param attention_mask: chex.Array: Mask out the attention weights for certain positions</span>
<span class="sd">    :param causal_mask: chex.Array: Mask the future tokens</span>
<span class="sd">    :param position_ids: chex.Array: Indicate the position of each token in the sequence</span>
<span class="sd">    :param deterministic: bool: Determine whether to use dropout or not</span>
<span class="sd">    :param init_cache: bool: Initialize the cache for the self-attention layer</span>
<span class="sd">    :param output_attentions: bool: Determine whether to return the attention weights or not</span>
<span class="sd">    :return: A tuple of hidden_states and attention_output</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_layernorm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span><span class="p">,</span> <span class="n">self_attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span>
        <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">freq_cis</span><span class="o">=</span><span class="n">freq_cis</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">causal_mask</span><span class="o">=</span><span class="n">causal_mask</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
        <span class="n">deterministic</span><span class="o">=</span><span class="n">deterministic</span><span class="p">,</span>
        <span class="n">init_cache</span><span class="o">=</span><span class="n">init_cache</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span>
    <span class="p">)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

    <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_attention_layernorm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span><span class="p">,</span> <span class="n">router_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_sparse_moe</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
    <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">self_attn_weights</span><span class="p">,)</span>
    <span class="k">if</span> <span class="n">output_router_logits</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">router_logits</span><span class="p">,)</span>
    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayerCollection" class="doc doc-heading">
          <code>FlaxMixtralDecoderLayerCollection</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="flax.linen.Module">Module</span></code></p>


            <details class="quote">
              <summary>Source code in <code>lib/python/EasyDel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">FlaxMixtralDecoderLayerCollection</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">MixtralConfig</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="n">param_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="n">precision</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">Precision</span><span class="p">]</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">Precision</span><span class="p">(</span><span class="s2">&quot;fastest&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">FlaxMixtralDecoderLayer</span><span class="p">(</span>
                <span class="n">layer_index</span><span class="o">=</span><span class="n">layer_index</span><span class="p">,</span>
                <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">param_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dtype</span><span class="p">,</span>
                <span class="n">precision</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">layer_index</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="k">for</span> <span class="n">layer_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">freq_cis</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">causal_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">deterministic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
            <span class="n">init_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">output_router_logits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The __call__ function is the main function of a TransformerEncoderLayer.</span>
<span class="sd">        It takes in the following arguments:</span>
<span class="sd">            hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.</span>
<span class="sd">            freq_cis (chex.Array): A tensor containing frequency-domain representations of each token&#39;s context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num</span>

<span class="sd">        :param self: Represent the instance of the class</span>
<span class="sd">        :param hidden_states: chex.Array: Represent the input to the encoder layer</span>
<span class="sd">        :param freq_cis: chex.Array: Pass the frequency information to the attention layer</span>
<span class="sd">        :param attention_mask: chex.Array: Mask out the attention weights for certain positions</span>
<span class="sd">        :param causal_mask: chex.Array: Mask the future tokens</span>
<span class="sd">        :param position_ids: chex.Array: Indicate the position of each token in the sequence</span>
<span class="sd">        :param deterministic: bool: Determine whether to use dropout or not</span>
<span class="sd">        :param init_cache: bool: Initialize the cache for the self-attention layer</span>
<span class="sd">        :param output_attentions: bool: Determine whether to return the attention weights or not</span>
<span class="sd">        :return: A tuple of hidden_states, attention_output, all_hidden_states and all_router_logits</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">all_self_attns</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">all_router_logits</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_router_logits</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                <span class="n">all_hidden_states</span> <span class="o">+=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
            <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">output_router_logits</span><span class="o">=</span><span class="n">output_router_logits</span><span class="p">,</span>
                <span class="n">init_cache</span><span class="o">=</span><span class="n">init_cache</span><span class="p">,</span>
                <span class="n">freq_cis</span><span class="o">=</span><span class="n">freq_cis</span><span class="p">,</span>
                <span class="n">causal_mask</span><span class="o">=</span><span class="n">causal_mask</span><span class="p">,</span>
                <span class="n">deterministic</span><span class="o">=</span><span class="n">deterministic</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                <span class="n">all_self_attns</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],)</span>

            <span class="k">if</span> <span class="n">output_router_logits</span><span class="p">:</span>
                <span class="n">all_router_logits</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">all_self_attns</span><span class="p">,)</span>
        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">all_hidden_states</span><span class="p">,)</span>
        <span class="k">if</span> <span class="n">output_router_logits</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">all_router_logits</span><span class="p">,)</span>
        <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayerCollection.__call__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">freq_cis</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">causal_mask</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">init_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">output_router_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>The <strong>call</strong> function is the main function of a TransformerEncoderLayer.
It takes in the following arguments:
    hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.
    freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>self</code></td>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Represent the instance of the class</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>hidden_states</code></td>
          <td>
                <code><span title="chex.Array">Array</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>chex.Array: Represent the input to the encoder layer</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>freq_cis</code></td>
          <td>
                <code><span title="chex.Array">Array</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>chex.Array: Pass the frequency information to the attention layer</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>attention_mask</code></td>
          <td>
                <code><span title="chex.Array">Array</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>chex.Array: Mask out the attention weights for certain positions</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>causal_mask</code></td>
          <td>
                <code><span title="chex.Array">Array</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>chex.Array: Mask the future tokens</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>position_ids</code></td>
          <td>
                <code><span title="chex.Array">Array</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>chex.Array: Indicate the position of each token in the sequence</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>deterministic</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>bool: Determine whether to use dropout or not</p>
            </div>
          </td>
          <td>
                <code>True</code>
          </td>
        </tr>
        <tr>
          <td><code>init_cache</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>bool: Initialize the cache for the self-attention layer</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>output_attentions</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[bool]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>bool: Determine whether to return the attention weights or not</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A tuple of hidden_states, attention_output, all_hidden_states and all_router_logits</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>lib/python/EasyDel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">freq_cis</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">causal_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">deterministic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">init_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">output_router_logits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The __call__ function is the main function of a TransformerEncoderLayer.</span>
<span class="sd">    It takes in the following arguments:</span>
<span class="sd">        hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.</span>
<span class="sd">        freq_cis (chex.Array): A tensor containing frequency-domain representations of each token&#39;s context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num</span>

<span class="sd">    :param self: Represent the instance of the class</span>
<span class="sd">    :param hidden_states: chex.Array: Represent the input to the encoder layer</span>
<span class="sd">    :param freq_cis: chex.Array: Pass the frequency information to the attention layer</span>
<span class="sd">    :param attention_mask: chex.Array: Mask out the attention weights for certain positions</span>
<span class="sd">    :param causal_mask: chex.Array: Mask the future tokens</span>
<span class="sd">    :param position_ids: chex.Array: Indicate the position of each token in the sequence</span>
<span class="sd">    :param deterministic: bool: Determine whether to use dropout or not</span>
<span class="sd">    :param init_cache: bool: Initialize the cache for the self-attention layer</span>
<span class="sd">    :param output_attentions: bool: Determine whether to return the attention weights or not</span>
<span class="sd">    :return: A tuple of hidden_states, attention_output, all_hidden_states and all_router_logits</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">all_self_attns</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">all_router_logits</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_router_logits</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">all_hidden_states</span> <span class="o">+=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
        <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_router_logits</span><span class="o">=</span><span class="n">output_router_logits</span><span class="p">,</span>
            <span class="n">init_cache</span><span class="o">=</span><span class="n">init_cache</span><span class="p">,</span>
            <span class="n">freq_cis</span><span class="o">=</span><span class="n">freq_cis</span><span class="p">,</span>
            <span class="n">causal_mask</span><span class="o">=</span><span class="n">causal_mask</span><span class="p">,</span>
            <span class="n">deterministic</span><span class="o">=</span><span class="n">deterministic</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="n">all_self_attns</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],)</span>

        <span class="k">if</span> <span class="n">output_router_logits</span><span class="p">:</span>
            <span class="n">all_router_logits</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],)</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
    <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">all_self_attns</span><span class="p">,)</span>
    <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">all_hidden_states</span><span class="p">,)</span>
    <span class="k">if</span> <span class="n">output_router_logits</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">all_router_logits</span><span class="p">,)</span>
    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralForCausalLM" class="doc doc-heading">
          <code>FlaxMixtralForCausalLM</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel" href="#lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel">MixtralPreTrainedModel</a></code></p>


            <details class="quote">
              <summary>Source code in <code>lib/python/EasyDel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">FlaxMixtralForCausalLM</span><span class="p">(</span><span class="n">MixtralPreTrainedModel</span><span class="p">):</span>
    <span class="n">module_class</span> <span class="o">=</span> <span class="n">FlaxMixtralForCausalLMModule</span>

    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed_tokens</span>

    <span class="k">def</span> <span class="nf">set_decoder</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">decoder</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">decoder</span>

    <span class="k">def</span> <span class="nf">get_decoder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">model</span>

    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">lm_head</span>

    <span class="k">def</span> <span class="nf">set_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">new_embeddings</span>

    <span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.</span>

<span class="sd">        :param self: Access variables that belong to the class</span>
<span class="sd">        :param input_ids: Pass in the input tokens</span>
<span class="sd">        :param max_length: Set the length of the sequence to be generated</span>
<span class="sd">        :param attention_mask: Optional[chex.Array]: Mask the attention weights</span>
<span class="sd">        :return: A dictionary of the past_key_values, attention_mask and position ids</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">past_key_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_cache</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">)</span>
        <span class="n">extended_attention_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
            <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position_ids</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="n">extended_attention_mask</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">dynamic_update_slice</span><span class="p">(</span>
                <span class="n">extended_attention_mask</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">position_ids</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">)[</span>
                                            <span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">))</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;past_key_values&quot;</span><span class="p">:</span> <span class="n">past_key_values</span><span class="p">,</span>
            <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">extended_attention_mask</span><span class="p">,</span>
            <span class="s2">&quot;position_ids&quot;</span><span class="p">:</span> <span class="n">position_ids</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">update_inputs_for_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_outputs</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">):</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_outputs</span><span class="o">.</span><span class="n">past_key_values</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">model_kwargs</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralForCausalLM.prepare_inputs_for_generation" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>self</code></td>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Access variables that belong to the class</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>input_ids</code></td>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Pass in the input tokens</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>max_length</code></td>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Set the length of the sequence to be generated</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>attention_mask</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="chex.Array">Array</span>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Optional[chex.Array]: Mask the attention weights</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A dictionary of the past_key_values, attention_mask and position ids</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>lib/python/EasyDel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.</span>

<span class="sd">    :param self: Access variables that belong to the class</span>
<span class="sd">    :param input_ids: Pass in the input tokens</span>
<span class="sd">    :param max_length: Set the length of the sequence to be generated</span>
<span class="sd">    :param attention_mask: Optional[chex.Array]: Mask the attention weights</span>
<span class="sd">    :return: A dictionary of the past_key_values, attention_mask and position ids</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>

    <span class="n">past_key_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_cache</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">)</span>
    <span class="n">extended_attention_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
        <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">position_ids</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">extended_attention_mask</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">dynamic_update_slice</span><span class="p">(</span>
            <span class="n">extended_attention_mask</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">position_ids</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">)[</span>
                                        <span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;past_key_values&quot;</span><span class="p">:</span> <span class="n">past_key_values</span><span class="p">,</span>
        <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">extended_attention_mask</span><span class="p">,</span>
        <span class="s2">&quot;position_ids&quot;</span><span class="p">:</span> <span class="n">position_ids</span><span class="p">,</span>
    <span class="p">}</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralSparseMoeBlock" class="doc doc-heading">
          <code>FlaxMixtralSparseMoeBlock</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="flax.linen.Module">Module</span></code></p>

  
      <p>This implementation is
strictly equivalent to standard MoE with full capacity (no
dropped tokens). It's faster since it formulates MoE operations
in terms of block-sparse operations to accomodate imbalanced
assignments of tokens to experts, whereas standard MoE either
(1) drop tokens at the cost of reduced performance or (2) set
capacity factor to number of experts and thus waste computation
and memory on padding.</p>

            <details class="quote">
              <summary>Source code in <code>lib/python/EasyDel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">FlaxMixtralSparseMoeBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This implementation is</span>
<span class="sd">    strictly equivalent to standard MoE with full capacity (no</span>
<span class="sd">    dropped tokens). It&#39;s faster since it formulates MoE operations</span>
<span class="sd">    in terms of block-sparse operations to accomodate imbalanced</span>
<span class="sd">    assignments of tokens to experts, whereas standard MoE either</span>
<span class="sd">    (1) drop tokens at the cost of reduced performance or (2) set</span>
<span class="sd">    capacity factor to number of experts and thus waste computation</span>
<span class="sd">    and memory on padding.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">MixtralConfig</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="n">param_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="n">precision</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">Precision</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">Precision</span><span class="p">(</span><span class="s2">&quot;fastest&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gate</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_local_experts</span><span class="p">,</span>
            <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">param_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dtype</span><span class="p">,</span>
            <span class="n">precision</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span>
            <span class="n">kernel_init</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">normal</span><span class="p">(),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">experts</span> <span class="o">=</span> <span class="n">FlaxMixtralBlocKSparesTop2MLPCollection</span><span class="p">(</span>
            <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">param_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dtype</span><span class="p">,</span>
            <span class="n">precision</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">]:</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>

        <span class="n">router_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">promote_types</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">routing_weights</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span>
            <span class="n">router_logits</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span>
                <span class="n">jnp</span><span class="o">.</span><span class="n">promote_types</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">routing_weights</span><span class="p">,</span> <span class="n">selected_experts</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">top_k</span><span class="p">(</span>
            <span class="n">routing_weights</span><span class="p">,</span>
            <span class="n">k</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_experts_per_tok</span>
        <span class="p">)</span>
        <span class="n">routing_weights</span> <span class="o">/=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
            <span class="n">routing_weights</span><span class="p">,</span>
            <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="n">routing_weights</span> <span class="o">=</span> <span class="n">routing_weights</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>
        <span class="n">expert_mask</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span>
            <span class="n">selected_experts</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_local_experts</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">experts</span><span class="p">(</span>
            <span class="n">expert_mask</span><span class="o">=</span><span class="n">expert_mask</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">sequence_length</span><span class="o">=</span><span class="n">sequence_length</span><span class="p">,</span>
            <span class="n">hidden_dim</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">routing_weights</span><span class="o">=</span><span class="n">routing_weights</span>
        <span class="p">),</span> <span class="n">router_logits</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">











  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel" class="doc doc-heading">
          <code>MixtralPreTrainedModel</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="lib.python.EasyDel.modules.easydel_modelling_utils.EasyDelFlaxPretrainedModel" href="../generated-modules-easydel_modelling_utils/#lib.python.EasyDel.modules.easydel_modelling_utils.EasyDelFlaxPretrainedModel">EasyDelFlaxPretrainedModel</a></code></p>


            <details class="quote">
              <summary>Source code in <code>lib/python/EasyDel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span>
<span class="normal">903</span>
<span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span>
<span class="normal">911</span>
<span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span>
<span class="normal">915</span>
<span class="normal">916</span>
<span class="normal">917</span>
<span class="normal">918</span>
<span class="normal">919</span>
<span class="normal">920</span>
<span class="normal">921</span>
<span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span>
<span class="normal">934</span>
<span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span>
<span class="normal">940</span>
<span class="normal">941</span>
<span class="normal">942</span>
<span class="normal">943</span>
<span class="normal">944</span>
<span class="normal">945</span>
<span class="normal">946</span>
<span class="normal">947</span>
<span class="normal">948</span>
<span class="normal">949</span>
<span class="normal">950</span>
<span class="normal">951</span>
<span class="normal">952</span>
<span class="normal">953</span>
<span class="normal">954</span>
<span class="normal">955</span>
<span class="normal">956</span>
<span class="normal">957</span>
<span class="normal">958</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MixtralPreTrainedModel</span><span class="p">(</span><span class="n">EasyDelFlaxPretrainedModel</span><span class="p">):</span>
    <span class="n">config_class</span><span class="p">:</span> <span class="n">MixtralConfig</span> <span class="o">=</span> <span class="n">MixtralConfig</span>
    <span class="n">module_class</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;model&quot;</span>

    <span class="c1"># main_input_name = &quot;input_ids&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">config</span><span class="p">:</span> <span class="n">MixtralConfig</span><span class="p">,</span>
            <span class="n">dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
            <span class="n">param_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
            <span class="n">precision</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">Precision</span><span class="p">]</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">Precision</span><span class="p">(</span>
                <span class="s2">&quot;fastest&quot;</span><span class="p">),</span>
            <span class="n">input_shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
            <span class="n">_do_init</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_class</span><span class="p">(</span>
            <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">param_dtype</span><span class="o">=</span><span class="n">param_dtype</span><span class="p">,</span>
            <span class="n">precision</span><span class="o">=</span><span class="n">precision</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">_do_init</span><span class="o">=</span><span class="n">_do_init</span><span class="p">,</span>
            <span class="n">module</span><span class="o">=</span><span class="n">module</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span>
            <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">rng</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">,</span>
            <span class="n">input_shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">,</span>
            <span class="n">params</span><span class="p">:</span> <span class="n">FrozenDict</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">FrozenDict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The init_weights function is used to initialize the weights of a model.</span>
<span class="sd">        It takes in a rng, which is a random number generator key that can be used to generate random numbers.</span>
<span class="sd">        The input_shape parameter specifies the shape of the inputs that will be fed into this model.</span>
<span class="sd">        The params parameter allows you to pass in pre-trained weights for your model, if you have them available.</span>

<span class="sd">        :param self: Access variables that belong to the class</span>
<span class="sd">        :param rng: jax.random.PRNGKey: Initialize the weights of the model</span>
<span class="sd">        :param input_shape: Tuple: Initialize the input_ids, attention_mask and position_ids</span>
<span class="sd">        :param params: flax.core.FrozenDict: Pass in the parameters of a pre-trained model</span>
<span class="sd">        :return: A frozendict of parameters</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">initialization_of_moe</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">)</span>
        <span class="n">position_ids</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">),</span>
            <span class="n">input_shape</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">params_rng</span><span class="p">,</span> <span class="n">dropout_rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
        <span class="n">rngs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="n">params_rng</span><span class="p">,</span> <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="n">dropout_rng</span><span class="p">}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">add_cross_attention</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="n">input_shape</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,))</span>
            <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span>
            <span class="n">module_init_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">init</span><span class="p">(</span>
                <span class="n">rngs</span><span class="p">,</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">position_ids</span><span class="p">,</span>
                <span class="n">encoder_hidden_states</span><span class="p">,</span>
                <span class="n">encoder_attention_mask</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">module_init_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">init</span><span class="p">(</span>
                <span class="n">rngs</span><span class="p">,</span>
                <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>
        <span class="n">random_params</span> <span class="o">=</span> <span class="n">module_init_outputs</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">initialization_of_moe</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">random_params</span> <span class="o">=</span> <span class="n">flatten_dict</span><span class="p">(</span><span class="n">unfreeze</span><span class="p">(</span><span class="n">random_params</span><span class="p">))</span>
            <span class="n">params</span> <span class="o">=</span> <span class="n">flatten_dict</span><span class="p">(</span><span class="n">unfreeze</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">missing_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_missing_keys</span><span class="p">:</span>
                <span class="n">params</span><span class="p">[</span><span class="n">missing_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">random_params</span><span class="p">[</span><span class="n">missing_key</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_missing_keys</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">freeze</span><span class="p">(</span><span class="n">unflatten_dict</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">random_params</span>

    <span class="k">def</span> <span class="nf">init_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">):</span>

        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">))</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="n">position_ids</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">init_variables</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">init</span><span class="p">(</span>
            <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">init_cache</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">init_variables</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">input_ids</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">params</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">dropout_rng</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">train</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">output_router_logits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">add_params_field</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The __call__ function is the main function of a JAX module.</span>
<span class="sd">        It takes as input:</span>
<span class="sd">        - The parameters of the model (self.params)</span>
<span class="sd">        - The inputs to the model (input_ids, attention_mask, position_ids)</span>
<span class="sd">        - Whether we are training (train=True/False) and whether we want to return all hidden states and</span>
<span class="sd">        attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).</span>

<span class="sd">        :param self: Represent the instance of the class</span>
<span class="sd">        :param input_ids: Pass the input sequence to the model</span>
<span class="sd">        :param attention_mask: Mask out the padding tokens</span>
<span class="sd">        :param position_ids: Specify the position of each token in the sequence</span>
<span class="sd">        :param params: dict: Pass in the parameters of the model</span>
<span class="sd">        :param past_key_values: dict: Pass the past key values to the model</span>
<span class="sd">        :param dropout_rng: jax.random.PRNGKey: Pass in a random number generator key to the model</span>
<span class="sd">        :param train: bool: Determine whether to use dropout or not</span>
<span class="sd">        :param output_attentions: Optional[bool]: Determine whether to return the attention weights</span>
<span class="sd">        :param output_hidden_states: Optional[bool]: Determine whether to return the hidden states of all layers</span>
<span class="sd">        :param return_dict: Optional[bool]: Return a dictionary of the outputs</span>
<span class="sd">        :param add_params_field: bool: Add a params field to the inputs dictionary</span>
<span class="sd">        :return: A tuple of (last_hidden_state, past_key_values)</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># TODO: Here needs to be fixed</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">return_dict</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="n">position_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Make sure to provide `position_ids` when passing `past_key_values`.&quot;</span><span class="p">)</span>

            <span class="n">position_ids</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">)[</span>
                                            <span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">))</span>

        <span class="n">rng_s</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">dropout_rng</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">rng_s</span><span class="p">[</span><span class="s2">&quot;dropout&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dropout_rng</span>

        <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="n">params</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">}</span> <span class="k">if</span> <span class="n">add_params_field</span> <span class="k">else</span> <span class="n">params</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">bits</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">rng_s</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">past_key_values</span><span class="p">:</span>
            <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">past_key_values</span>
            <span class="n">mutable</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">mutable</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">),</span>  <span class="c1"># input_ids: chex.Array</span>
            <span class="c1"># attention_mask: Optional[chex.Array] = None</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">),</span>
            <span class="c1"># position_ids: Optional[chex.Array] = None</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">position_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">),</span>
            <span class="kc">None</span><span class="p">,</span>  <span class="c1"># inputs_embeds: Optional[chex.Array] = None</span>
            <span class="n">output_attentions</span><span class="p">,</span>  <span class="c1"># output_attentions: Optional[bool] = None</span>
            <span class="c1"># output_hidden_states: Optional[bool] = None</span>
            <span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="c1"># output_router_logits: Optional[bool] = None</span>
            <span class="n">output_router_logits</span><span class="p">,</span>
            <span class="kc">False</span><span class="p">,</span>  <span class="c1"># init_cache: bool = False</span>
            <span class="ow">not</span> <span class="n">train</span><span class="p">,</span>  <span class="c1"># deterministic: bool = True</span>
            <span class="n">return_dict</span><span class="p">,</span>  <span class="c1"># return_dict: bool = True</span>
            <span class="n">rngs</span><span class="o">=</span><span class="n">rng_s</span><span class="p">,</span>
            <span class="n">mutable</span><span class="o">=</span><span class="n">mutable</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">outputs</span><span class="p">,</span> <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">outputs</span>
            <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">unfreeze</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">outputs</span>
        <span class="k">elif</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">outputs</span><span class="p">,</span> <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">outputs</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> \
                <span class="p">(</span><span class="n">unfreeze</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">]),)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

        <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel.__call__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout_rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_router_logits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">add_params_field</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>The <strong>call</strong> function is the main function of a JAX module.
It takes as input:
- The parameters of the model (self.params)
- The inputs to the model (input_ids, attention_mask, position_ids)
- Whether we are training (train=True/False) and whether we want to return all hidden states and
attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>self</code></td>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Represent the instance of the class</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>input_ids</code></td>
          <td>
                <code><span title="chex.Array">Array</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Pass the input sequence to the model</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>attention_mask</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="chex.Array">Array</span>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Mask out the padding tokens</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>position_ids</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="chex.Array">Array</span>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Specify the position of each token in the sequence</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>params</code></td>
          <td>
                <code>dict</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>dict: Pass in the parameters of the model</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>past_key_values</code></td>
          <td>
                <code>dict</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>dict: Pass the past key values to the model</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>dropout_rng</code></td>
          <td>
                <code><span title="jax.random.PRNGKey">PRNGKey</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>jax.random.PRNGKey: Pass in a random number generator key to the model</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>train</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>bool: Determine whether to use dropout or not</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>output_attentions</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[bool]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Optional[bool]: Determine whether to return the attention weights</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>output_hidden_states</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[bool]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Optional[bool]: Determine whether to return the hidden states of all layers</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>return_dict</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[bool]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Optional[bool]: Return a dictionary of the outputs</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>add_params_field</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>bool: Add a params field to the inputs dictionary</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A tuple of (last_hidden_state, past_key_values)</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>lib/python/EasyDel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span>
<span class="normal">903</span>
<span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span>
<span class="normal">911</span>
<span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span>
<span class="normal">915</span>
<span class="normal">916</span>
<span class="normal">917</span>
<span class="normal">918</span>
<span class="normal">919</span>
<span class="normal">920</span>
<span class="normal">921</span>
<span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span>
<span class="normal">934</span>
<span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span>
<span class="normal">940</span>
<span class="normal">941</span>
<span class="normal">942</span>
<span class="normal">943</span>
<span class="normal">944</span>
<span class="normal">945</span>
<span class="normal">946</span>
<span class="normal">947</span>
<span class="normal">948</span>
<span class="normal">949</span>
<span class="normal">950</span>
<span class="normal">951</span>
<span class="normal">952</span>
<span class="normal">953</span>
<span class="normal">954</span>
<span class="normal">955</span>
<span class="normal">956</span>
<span class="normal">957</span>
<span class="normal">958</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">params</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dropout_rng</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">train</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_router_logits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_params_field</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The __call__ function is the main function of a JAX module.</span>
<span class="sd">    It takes as input:</span>
<span class="sd">    - The parameters of the model (self.params)</span>
<span class="sd">    - The inputs to the model (input_ids, attention_mask, position_ids)</span>
<span class="sd">    - Whether we are training (train=True/False) and whether we want to return all hidden states and</span>
<span class="sd">    attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).</span>

<span class="sd">    :param self: Represent the instance of the class</span>
<span class="sd">    :param input_ids: Pass the input sequence to the model</span>
<span class="sd">    :param attention_mask: Mask out the padding tokens</span>
<span class="sd">    :param position_ids: Specify the position of each token in the sequence</span>
<span class="sd">    :param params: dict: Pass in the parameters of the model</span>
<span class="sd">    :param past_key_values: dict: Pass the past key values to the model</span>
<span class="sd">    :param dropout_rng: jax.random.PRNGKey: Pass in a random number generator key to the model</span>
<span class="sd">    :param train: bool: Determine whether to use dropout or not</span>
<span class="sd">    :param output_attentions: Optional[bool]: Determine whether to return the attention weights</span>
<span class="sd">    :param output_hidden_states: Optional[bool]: Determine whether to return the hidden states of all layers</span>
<span class="sd">    :param return_dict: Optional[bool]: Return a dictionary of the outputs</span>
<span class="sd">    :param add_params_field: bool: Add a params field to the inputs dictionary</span>
<span class="sd">    :return: A tuple of (last_hidden_state, past_key_values)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># TODO: Here needs to be fixed</span>
    <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
    <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
    <span class="p">)</span>
    <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">return_dict</span>

    <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>

    <span class="k">if</span> <span class="n">position_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Make sure to provide `position_ids` when passing `past_key_values`.&quot;</span><span class="p">)</span>

        <span class="n">position_ids</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">)[</span>
                                        <span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">))</span>

    <span class="n">rng_s</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="n">dropout_rng</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">rng_s</span><span class="p">[</span><span class="s2">&quot;dropout&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dropout_rng</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="n">params</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">}</span> <span class="k">if</span> <span class="n">add_params_field</span> <span class="k">else</span> <span class="n">params</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">bits</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">rng_s</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">past_key_values</span><span class="p">:</span>
        <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">past_key_values</span>
        <span class="n">mutable</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">mutable</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">),</span>  <span class="c1"># input_ids: chex.Array</span>
        <span class="c1"># attention_mask: Optional[chex.Array] = None</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">),</span>
        <span class="c1"># position_ids: Optional[chex.Array] = None</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">position_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">),</span>
        <span class="kc">None</span><span class="p">,</span>  <span class="c1"># inputs_embeds: Optional[chex.Array] = None</span>
        <span class="n">output_attentions</span><span class="p">,</span>  <span class="c1"># output_attentions: Optional[bool] = None</span>
        <span class="c1"># output_hidden_states: Optional[bool] = None</span>
        <span class="n">output_hidden_states</span><span class="p">,</span>
        <span class="c1"># output_router_logits: Optional[bool] = None</span>
        <span class="n">output_router_logits</span><span class="p">,</span>
        <span class="kc">False</span><span class="p">,</span>  <span class="c1"># init_cache: bool = False</span>
        <span class="ow">not</span> <span class="n">train</span><span class="p">,</span>  <span class="c1"># deterministic: bool = True</span>
        <span class="n">return_dict</span><span class="p">,</span>  <span class="c1"># return_dict: bool = True</span>
        <span class="n">rngs</span><span class="o">=</span><span class="n">rng_s</span><span class="p">,</span>
        <span class="n">mutable</span><span class="o">=</span><span class="n">mutable</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">outputs</span>
        <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">unfreeze</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">outputs</span>
    <span class="k">elif</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">outputs</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> \
            <span class="p">(</span><span class="n">unfreeze</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">]),)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="lib.python.EasyDel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel.init_weights" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">init_weights</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>The init_weights function is used to initialize the weights of a model.
It takes in a rng, which is a random number generator key that can be used to generate random numbers.
The input_shape parameter specifies the shape of the inputs that will be fed into this model.
The params parameter allows you to pass in pre-trained weights for your model, if you have them available.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>self</code></td>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Access variables that belong to the class</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>rng</code></td>
          <td>
                <code><span title="jax.random.PRNGKey">PRNGKey</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>jax.random.PRNGKey: Initialize the weights of the model</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>input_shape</code></td>
          <td>
                <code><span title="typing.Tuple">Tuple</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tuple: Initialize the input_ids, attention_mask and position_ids</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>params</code></td>
          <td>
                <code><span title="flax.core.FrozenDict">FrozenDict</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>flax.core.FrozenDict: Pass in the parameters of a pre-trained model</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="flax.core.FrozenDict">FrozenDict</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A frozendict of parameters</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>lib/python/EasyDel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">rng</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">,</span>
        <span class="n">input_shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">,</span>
        <span class="n">params</span><span class="p">:</span> <span class="n">FrozenDict</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">FrozenDict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The init_weights function is used to initialize the weights of a model.</span>
<span class="sd">    It takes in a rng, which is a random number generator key that can be used to generate random numbers.</span>
<span class="sd">    The input_shape parameter specifies the shape of the inputs that will be fed into this model.</span>
<span class="sd">    The params parameter allows you to pass in pre-trained weights for your model, if you have them available.</span>

<span class="sd">    :param self: Access variables that belong to the class</span>
<span class="sd">    :param rng: jax.random.PRNGKey: Initialize the weights of the model</span>
<span class="sd">    :param input_shape: Tuple: Initialize the input_ids, attention_mask and position_ids</span>
<span class="sd">    :param params: flax.core.FrozenDict: Pass in the parameters of a pre-trained model</span>
<span class="sd">    :return: A frozendict of parameters</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">initialization_of_moe</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">)</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">)</span>
    <span class="n">position_ids</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">),</span>
        <span class="n">input_shape</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">params_rng</span><span class="p">,</span> <span class="n">dropout_rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
    <span class="n">rngs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="n">params_rng</span><span class="p">,</span> <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="n">dropout_rng</span><span class="p">}</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">add_cross_attention</span><span class="p">:</span>
        <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="n">input_shape</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,))</span>
        <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span>
        <span class="n">module_init_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">init</span><span class="p">(</span>
            <span class="n">rngs</span><span class="p">,</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attention_mask</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">module_init_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">init</span><span class="p">(</span>
            <span class="n">rngs</span><span class="p">,</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
    <span class="n">random_params</span> <span class="o">=</span> <span class="n">module_init_outputs</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">initialization_of_moe</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">random_params</span> <span class="o">=</span> <span class="n">flatten_dict</span><span class="p">(</span><span class="n">unfreeze</span><span class="p">(</span><span class="n">random_params</span><span class="p">))</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">flatten_dict</span><span class="p">(</span><span class="n">unfreeze</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">missing_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_missing_keys</span><span class="p">:</span>
            <span class="n">params</span><span class="p">[</span><span class="n">missing_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">random_params</span><span class="p">[</span><span class="n">missing_key</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_missing_keys</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">freeze</span><span class="p">(</span><span class="n">unflatten_dict</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">random_params</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>




  </div>

  </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Erfan Zare Chavoshi-EasyDel
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.caa56a14.min.js"></script>
      
    
  </body>
</html>